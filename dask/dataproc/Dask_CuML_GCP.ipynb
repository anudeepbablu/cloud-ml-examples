{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cultural-conversation",
   "metadata": {},
   "source": [
    "# Using Dask and CuML with Google Cloud Dataproc\n",
    "\n",
    "In this workshop, you will learn the use of Dask and CuML on Dataptoc.\n",
    "\n",
    "__Dask__ is an open source library for parallel computing written in Python. Dask framework enables us to have a scheduler and a bunch of workers. You submit tasks to the scheduler and it automatically distributes the work among the workers. It works exceptionally well on a single machine, and can scale out to large clusters when needed. \n",
    "<img src=\"./images/dask-logo.png\" width=\"200\" height=\"200\">\n",
    "\n",
    "__RAPIDS CuML__ is a suite of fast, GPU-accelerated machine learning algorithms designed for data science and analytical tasks. Our API mirrors Sklearn’s, and we provide practitioners with the easy fit-predict-transform paradigm without ever having to program on a GPU.\n",
    "<img src=\"./images/rapids_cuml.png\" width=\"300\" height=\"200\">\n",
    "\n",
    "__Dataproc__ is a fully managed and highly scalable service for running Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. It is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data. \n",
    "<img src=\"./images/gcp_dataproc_logo.png\" width=\"400\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-extension",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "GPUs can greatly accelerate all stages of an ML pipeline: pre-processing, training, and inference. In this workshop, we will be focusing on the pre-processing and training stages, using Python in a Jupyter Notebook environment. First, we will use Dask/RAPIDS to read a dataset into NVIDIA GPU memory and execute some basic functions. Then, we’ll use Dask to scale beyond our GPU memory capacity.\n",
    "\n",
    "This notebook has following sections:\n",
    "\n",
    "* Introduction to Dataproc\n",
    "* Introduction to Dask\n",
    "* Data Loading\n",
    "* ETL\n",
    "* Introduction to CuML\n",
    "* Introduction to XGBoost\n",
    "* Machine Learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-promotion",
   "metadata": {},
   "source": [
    "## Introduction to Dask\n",
    "\n",
    "Dask is the most commonly used parallelism framework within the PyData and SciPy communities. Dask is designed to scale from parallelizing workloads on the CPUs in your laptop to thousands of nodes in a cloud cluster. In conjunction with the open-source RAPIDS framework developed by NVIDIA, you can utilize the parallel processing power of both CPUs and NVIDIA GPUs. \n",
    "\n",
    "In Dask programming, we create computational graphs that define code we **would like** to execute, and then, give these computational graphs to a Dask scheduler which evaluates them lazily, and efficiently, in parallel.\n",
    "\n",
    "In addition to using multiple CPU cores or threads to execute computational graphs in parallel, Dask schedulers can also be configured to execute computational graphs on multiple CPUs, or, as we will do in this workshop, multiple GPUs. As a result, Dask programming facilitates operating on datasets that are larger than the memory of a single compute resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import certifi\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from functools import partial\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "# Dask imports\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask_cudf\n",
    "\n",
    "from dask_kubernetes import KubeCluster, make_pod_from_dict\n",
    "from dask.distributed import WorkerPlugin, wait, progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-latin",
   "metadata": {},
   "source": [
    "## Nvidia GPUs\n",
    "Let us start by checking hardware available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-stress",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-arlington",
   "metadata": {},
   "source": [
    "Other details like Memory-Usage and GPU utilization can be shown using following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-moderator",
   "metadata": {},
   "source": [
    "### Starting a `LocalCUDACluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-racing",
   "metadata": {},
   "source": [
    "`dask_cuda` provides utilities for Dask and CUDA (the \"cu\" in cuDF) interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "cluster = LocalCUDACluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-sunglasses",
   "metadata": {},
   "source": [
    "## Instantiating a Client Connection\n",
    "The `dask.distributed` library gives us distributed functionality, including the ability to connect to the CUDA Cluster we just created. The `progress` import will give us a handy progress bar we can utilize below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-whole",
   "metadata": {},
   "source": [
    "Dask library is smart enough to choose number of works and cores by default. In case of GPUs, `number of workers == number of GPUs` by default. In a distrubuted cluster environment, you can choose number of workers and cores. Dask ships with a very helpful dashboard that in our case runs on port `8787`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPU Cluster status: ', client.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the client for all connected workers\n",
    "# Number of workers in out cluster\n",
    "workers = client.has_what().keys()\n",
    "n_workers = len(workers)\n",
    "print('Number of GPU workers: ', n_workers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-gauge",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are using [NYC Taxi Trip Duration Dataset from Kaggle](https://www.kaggle.com/c/nyc-taxi-trip-duration).\n",
    "\n",
    "### Data fields\n",
    "\n",
    "| Colonne            | Description                                                                                                                                                                                                           |\n",
    "|:-------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| __id__                 | a unique identifier for each trip                                                                                                                                                                                     |\n",
    "| __vendor_id__         | a code indicating the provider associated with the trip record                                                                                                                                                        |\n",
    "| __pickup_datetime__    | date and time when the meter was engaged                                                                                                                                                                              |\n",
    "| __dropoff_datetime__   | date and time when the meter was disengaged                                                                                                                                                                           |\n",
    "| __passenger_count__    | the number of passengers in the vehicle (driver entered value)                                                                                                                                                        |\n",
    "| __pickup_longitude__   | the longitude where the meter was engaged                                                                                                                                                                             |\n",
    "| __pickup_latitude__    | the latitude where the meter was engaged                                                                                                                                                                              |\n",
    "| __dropoff_longitude__  | the longitude where the meter was disengaged                                                                                                                                                                          |\n",
    "| __dropoff_latitude__   | the latitude where the meter was disengaged                                                                                                                                                                           |\n",
    "| __store_and_fwd_flag__ | This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server (Y=store and forward; N=not a store and forward trip) |\n",
    "| __trip_duration__      | duration of the trip in seconds                                                                                                                                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-drawing",
   "metadata": {},
   "source": [
    "## ETL Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-clear",
   "metadata": {},
   "source": [
    "### Taxi Data Configuration (Medium)\n",
    "We can use the parquet data from the anaconda public repo here. Which will illustrate how much faster it is to read parquet, and gives us around 150 million rows of data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_parquet_data_loader(client, response_dtype=np.float32, fraction=1.0, random_state=0):\n",
    "    \"\"\"\n",
    "    A Parquet data_loader. Reads parquet files, clean and process to return a dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    client: DASK client associated with the cluster we're interesting in collecting performance data for.\n",
    "    fraction: Fraction of axis items to return. Cannot be used with n\n",
    "    random_state: Seed for the random number generator (if int), or None. If None, a random seed will be chosen.\n",
    "                if RandomState, seed will be extracted from current state.\n",
    "\n",
    "    \"\"\"\n",
    "    # list of column names that need to be re-mapped\n",
    "    remap = {}\n",
    "    remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "    remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "    remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'pickup_datetime': 'datetime64[ms]',\n",
    "     'dropoff_datetime': 'datetime64[ms]',\n",
    "     'passenger_count': 'int32',\n",
    "     'trip_distance': 'float32',\n",
    "     'pickup_longitude': 'float32',\n",
    "     'pickup_latitude': 'float32',\n",
    "     'rate_code': 'int32',\n",
    "     'dropoff_longitude': 'float32',\n",
    "     'dropoff_latitude': 'float32',\n",
    "     'fare_amount': 'float32'\n",
    "    }\n",
    "\n",
    "    # apply a list of filter conditions to throw out records with missing or outlier values\n",
    "    query_frags = [\n",
    "        'fare_amount > 0 and fare_amount < 500',\n",
    "        'passenger_count > 0 and passenger_count < 6',\n",
    "        'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "        'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "        'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "        'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "    ]\n",
    "\n",
    "    workers = client.has_what().keys()\n",
    "    taxi_parquet_path = \"gs://anaconda-public-data/nyc-taxi/nyc.parquet\"\n",
    "    response_id = 'fare_amount'\n",
    "    fields = ['passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                 'dropoff_longitude', 'dropoff_latitude', 'fare_amount']\n",
    "    \n",
    "    taxi_df = dask_cudf.read_parquet(taxi_parquet_path, npartitions=len(workers))\n",
    "    taxi_df = clean(taxi_df, remap, must_haves)\n",
    "    taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "    taxi_df = taxi_df[fields]\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        taxi_df = client.persist(collections=taxi_df)\n",
    "    \n",
    "    wait(taxi_df)\n",
    "\n",
    "    X = taxi_df[taxi_df.columns.difference([response_id])].astype(np.float32)\n",
    "    y = taxi_df[response_id].astype(response_dtype)\n",
    "        \n",
    "    return taxi_df, X, y\n",
    "\n",
    "def clean(df_part, remap, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(columns=tmp)\n",
    "    \n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(columns=remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "            \n",
    "    return df_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names that need to be re-mapped\n",
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'pickup_datetime': 'datetime64[ms]',\n",
    " 'dropoff_datetime': 'datetime64[ms]',\n",
    " 'passenger_count': 'int32',\n",
    " 'trip_distance': 'float32',\n",
    " 'pickup_longitude': 'float32',\n",
    " 'pickup_latitude': 'float32',\n",
    " 'rate_code': 'int32',\n",
    " 'dropoff_longitude': 'float32',\n",
    " 'dropoff_latitude': 'float32',\n",
    " 'fare_amount': 'float32'\n",
    "}\n",
    "\n",
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is path to our dataset and it is a parquet file. Can we load a CSV file too? Oh yes! \n",
    "taxi_parquet_path = \"gs://anaconda-public-data/nyc-taxi/nyc.parquet\"\n",
    "\n",
    "# We are considering a limited columns for our ML pipeline.\n",
    "response_id = 'fare_amount'\n",
    "fields = ['passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "             'dropoff_longitude', 'dropoff_latitude', 'fare_amount']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-andorra",
   "metadata": {},
   "source": [
    "### Load Dataset\n",
    "By default, dask uses default block size of 64 MB each. So, `size of dataset / 64` will be number of tasks spread across partitions. If you desire to create a DataFrame with a specific block size, and use `npartitions` argument like below. We used `npartitions=len(workers)` to split data equally into all workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "taxi_df = dask_cudf.read_parquet(taxi_parquet_path, npartitions=len(workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = clean(taxi_df, remap, must_haves)\n",
    "taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "taxi_df = taxi_df[fields]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-sacrifice",
   "metadata": {},
   "source": [
    "__Woah!__ I thought we will see data. What do you think has happened here?\n",
    "\n",
    "Because Dask is lazy, the computation has not yet occurred. We can see that there are 160 tasks in the task graph. We can force computation by using `persist`. By forcing execution, the result is now explicitly in memory and our task graph only contains one task per partition (the baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dtype=np.float32\n",
    "\n",
    "with dask.annotate(workers=set(workers)):\n",
    "        taxi_df = client.persist(collections=taxi_df)\n",
    "    \n",
    "wait(taxi_df)\n",
    "\n",
    "X = taxi_df[taxi_df.columns.difference([response_id])].astype(np.float32)\n",
    "y = taxi_df[response_id].astype(response_dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-council",
   "metadata": {},
   "source": [
    "####  Split data to training and test set\n",
    "We will use the `train_test_split()` method from `Dask_ML`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                    train_size=0.80, \\\n",
    "                                                    random_state=42, \\\n",
    "                                                    shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.persist()\n",
    "X_test = X_test.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-charlotte",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a weighted vote of their predictions. There are three subsets of ensemble learning methods. \n",
    "\n",
    "1. __BAGGing__, or __B__ootstrap __AGG__regating\n",
    "2. __Boosting__\n",
    "3. __Stacking__\n",
    "\n",
    "If you like to get more details about it, please refer to [Simple Guide to Ensemble Methods](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2)\n",
    "\n",
    "\n",
    "### Random Forest Regressor\n",
    "\n",
    "The Decision Tree algorithm has a major disadvantage in that it causes over-fitting. To address these weaknesses, we turn to Random Forest. Random forest is a Supervised Learning algorithm which uses ensemble learning method for classification and regression. It is very fast and robust than other models. For anyone interested, we have added original papers and couple of blogs to learn more about Random Forest Regressor.\n",
    "\n",
    "We are going to use [RandomForestRegressor API](https://docs.rapids.ai/api/cuml/stable/api.html#random-forest) from CuML library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.dask.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_kwargs = {\n",
    "    \"workers\": client.has_what().keys(),\n",
    "    \"n_estimators\": 10,\n",
    "    \"max_depth\": 12\n",
    "}\n",
    "rf_csv_path = f\"./{out_prefix}_random_forest_regression.csv\"\n",
    "\n",
    "performance_sweep(client=client, model=RandomForestRegressor,\n",
    "                **sweep_kwargs,\n",
    "                out_path=rf_csv_path,\n",
    "                response_dtype=np.int32,\n",
    "                model_kwargs=rf_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_csv_path = f\"./{out_prefix}_random_forest_regression.csv\"\n",
    "visualize_csv_data(rf_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-translation",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost falls under the category of Boosting techniques in Ensemble Learning. The algorithm was developed to efficiently reduce computing time and allocate an optimal usage of memory resources. Important features of implementation include handling of missing values (Sparse Aware), Block Structure to support parallelization in tree construction and the ability to fit and boost on new data added to a trained model. ([reference](https://www.kdnuggets.com/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html)) \n",
    "\n",
    "Here is the original paper,\n",
    "[XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dmatrix_train = xgb.dask.DaskDMatrix(client, X_train, y_train)\n",
    "dmatrix_test = xgb.dask.DaskDMatrix(client, X_test, y_test)\n",
    "\n",
    "params = {\"max_depth\": 8,\n",
    "        \"max_leaves\": 2 ** 8,\n",
    "        \"alpha\": 0.9,\n",
    "        \"eta\": 0.1,\n",
    "        \"gamma\": 0.1,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 1,\n",
    "        \"reg_lambda\": 1,\n",
    "        \"scale_pos_weight\": 2,\n",
    "        \"min_child_weight\": 30,\n",
    "        \"tree_method\": \"gpu_hist\", #use GPU for tree building\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"grow_policy\": \"lossguide\",\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use xgboost.dask to distribute across workers-- returns dict \n",
    "xgb_clasf = xgb.dask.train(client, \n",
    "                           params,\n",
    "                           dmatrix_train, \n",
    "                           num_boost_round=100,\n",
    "                           evals=[(dmatrix_train, 'train'), (dmatrix_test,'test')]\n",
    "                          ) \n",
    "\n",
    "# booster is the trained model \n",
    "model_xgb = xgb_clasf['booster'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-original",
   "metadata": {},
   "source": [
    "### Inferencing\n",
    "#### Measuring model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to use GPU for inference.\n",
    "model_xgb.set_param({'predictor': 'gpu_predictor'})\n",
    "# dtrain is the DaskDMatrix defined above.\n",
    "y_predict = xgb.dask.predict(client, xgb_clasf['booster'], dmatrix_test)\n",
    "y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask \n",
    "y_test_c, y_predict = dask.compute(y_test, y_predict)\n",
    "print(\"XGB model AUC: {:.2%}\".format(cuml.metrics.roc_auc_score(y_test_c, y_predict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XGBoost model accuracy:   {:.2%}\".format(accuracy_score(y_test_c, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-physics",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "\n",
    "##### Dask\n",
    "\n",
    "##### XGBoost\n",
    "* [Ensemble Learning to Improve Machine Learning Results](https://blog.statsbot.co/ensemble-learning-d1dcd548e936)\n",
    "* [Complete Guide to Parameter Tuning in XGBoost with codes in Python](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "* [Understanding XGBoost Algorithm In Detail](https://analyticsindiamag.com/xgboost-internal-working-to-make-decision-trees-and-deduce-predictions/)\n",
    "\n",
    "##### Random Forest Regressor\n",
    "* [Random Forest](https://williamkoehrsen.medium.com/random-forest-simple-explanation-377895a60d2d) \\\n",
    "* [Random Forest Regression](https://towardsdatascience.com/machine-learning-basics-random-forest-regression-be3e1e3bb91a)\n",
    "* [Classification and Regression by randomForest](https://www.researchgate.net/profile/Andy-Liaw/publication/228451484_Classification_and_Regression_by_RandomForest/links/53fb24cc0cf20a45497047ab/Classification-and-Regression-by-RandomForest.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-scope",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
