{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cultural-conversation",
   "metadata": {},
   "source": [
    "# Using Dask and CuML with Google Cloud Dataproc\n",
    "\n",
    "In this workshop, you will learn the use of Dask and CuML on Dataptoc.\n",
    "\n",
    "__Dask__ is an open source library for parallel computing written in Python. Dask framework enables us to have a scheduler and a bunch of workers. You submit tasks to the scheduler and it automatically distributes the work among the workers. It works exceptionally well on a single machine, and can scale out to large clusters when needed. \n",
    "<img src=\"./images/dask-logo.png\" width=\"200\" height=\"200\">\n",
    "\n",
    "__RAPIDS CuML__ is a suite of fast, GPU-accelerated machine learning algorithms designed for data science and analytical tasks. Our API mirrors Sklearn’s, and we provide practitioners with the easy fit-predict-transform paradigm without ever having to program on a GPU.\n",
    "<img src=\"./images/rapids_cuml.png\" width=\"300\" height=\"200\">\n",
    "\n",
    "__Dataproc__ is a fully managed and highly scalable service for running Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks. It is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data. \n",
    "<img src=\"./images/gcp_dataproc_logo.png\" width=\"400\" height=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-extension",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "GPUs can greatly accelerate all stages of an ML pipeline: pre-processing, training, and inference. In this workshop, we will be focusing on the pre-processing and training stages, using Python in a Jupyter Notebook environment. First, we will use Dask/RAPIDS to read a dataset into NVIDIA GPU memory and execute some basic functions. Then, we’ll use Dask to scale beyond our GPU memory capacity.\n",
    "\n",
    "This notebook has following sections:\n",
    "\n",
    "* Introduction to Dataproc\n",
    "* Introduction to Dask\n",
    "* Data Loading\n",
    "* ETL\n",
    "* Introduction to CuML\n",
    "* Introduction to XGBoost\n",
    "* Machine Learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-promotion",
   "metadata": {},
   "source": [
    "## Introduction to Dask\n",
    "\n",
    "Dask is the most commonly used parallelism framework within the PyData and SciPy communities. Dask is designed to scale from parallelizing workloads on the CPUs in your laptop to thousands of nodes in a cloud cluster. In conjunction with the open-source RAPIDS framework developed by NVIDIA, you can utilize the parallel processing power of both CPUs and NVIDIA GPUs. \n",
    "\n",
    "In Dask programming, we create computational graphs that define code we **would like** to execute, and then, give these computational graphs to a Dask scheduler which evaluates them lazily, and efficiently, in parallel.\n",
    "\n",
    "In addition to using multiple CPU cores or threads to execute computational graphs in parallel, Dask schedulers can also be configured to execute computational graphs on multiple CPUs, or, as we will do in this workshop, multiple GPUs. As a result, Dask programming facilitates operating on datasets that are larger than the memory of a single compute resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-moderator",
   "metadata": {},
   "source": [
    "### Starting a `LocalCUDACluster`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-racing",
   "metadata": {},
   "source": [
    "`dask_cuda` provides utilities for Dask and CUDA (the \"cu\" in cuDF) interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "cluster = LocalCUDACluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-sunglasses",
   "metadata": {},
   "source": [
    "## Instantiating a Client Connection\n",
    "The `dask.distributed` library gives us distributed functionality, including the ability to connect to the CUDA Cluster we just created. The `progress` import will give us a handy progress bar we can utilize below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-whole",
   "metadata": {},
   "source": [
    "Dask ships with a very helpful dashboard that in our case runs on port `8787`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-gauge",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We are using [NYC Taxi Trip Duration Dataset from Kaggle](https://www.kaggle.com/c/nyc-taxi-trip-duration).\n",
    "\n",
    "### Data fields\n",
    "\n",
    "| Colonne            | Description                                                                                                                                                                                                           |\n",
    "|:-------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| __id__                 | a unique identifier for each trip                                                                                                                                                                                     |\n",
    "| __vendor_id__         | a code indicating the provider associated with the trip record                                                                                                                                                        |\n",
    "| __pickup_datetime__    | date and time when the meter was engaged                                                                                                                                                                              |\n",
    "| __dropoff_datetime__   | date and time when the meter was disengaged                                                                                                                                                                           |\n",
    "| __passenger_count__    | the number of passengers in the vehicle (driver entered value)                                                                                                                                                        |\n",
    "| __pickup_longitude__   | the longitude where the meter was engaged                                                                                                                                                                             |\n",
    "| __pickup_latitude__    | the latitude where the meter was engaged                                                                                                                                                                              |\n",
    "| __dropoff_longitude__  | the longitude where the meter was disengaged                                                                                                                                                                          |\n",
    "| __dropoff_latitude__   | the latitude where the meter was disengaged                                                                                                                                                                           |\n",
    "| __store_and_fwd_flag__ | This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server (Y=store and forward; N=not a store and forward trip) |\n",
    "| __trip_duration__      | duration of the trip in seconds                                                                                                                                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-clear",
   "metadata": {},
   "source": [
    "### Taxi Data Configuration (Medium)\n",
    "We can use the parquet data from the anaconda public repo here. Which will illustrate how much faster it is to read parquet, and gives us around 150 million rows of data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test with Taxi Dataset\n",
    "\n",
    "def data_loader():\n",
    "    \n",
    "    return\n",
    "\n",
    "preload_data = False\n",
    "append_to_existing = True\n",
    "samples = 5\n",
    "load_samples = 1\n",
    "worker_counts = [8]\n",
    "scaling_denom = 8\n",
    "hardware_type = 'V100'\n",
    "max_data_frac = 1.0\n",
    "scale_type = 'weak' # weak | strong\n",
    "out_prefix = 'taxi_medium'\n",
    "\n",
    "if (not preload_data):\n",
    "    data_loader = taxi_parquet_data_loader\n",
    "else:\n",
    "    data = taxi_parquet_data_loader(client, fraction=max_data_frac)\n",
    "    data_loader = lambda client, response_dtype, fraction, random_state: data\n",
    "\n",
    "print(data_loader)\n",
    "if (not hardware_type):\n",
    "    raise RuntimeError(\"Please specify the hardware type for this run! ex. (T4, V100, A100)\")\n",
    "\n",
    "sweep_kwargs = {\n",
    "    'append_to_existing': append_to_existing,\n",
    "    'samples': samples,\n",
    "    'load_samples': load_samples,\n",
    "    'worker_counts': worker_counts,\n",
    "    'scaling_denom': scaling_denom,\n",
    "    'hardware_type': hardware_type,\n",
    "    'data_loader': data_loader,\n",
    "    'max_data_frac': max_data_frac,\n",
    "    'scaling_type': scale_type\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import certifi\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from functools import partial\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask_cudf\n",
    "\n",
    "from dask.distributed import Client, WorkerPlugin, wait, progress\n",
    "\n",
    "class SimpleTimer:\n",
    "    def __init__(self):\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.elapsed = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter_ns()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time.perf_counter_ns()\n",
    "        self.elapsed = self.end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-twenty",
   "metadata": {},
   "source": [
    "### Taxi Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df_part, remap, must_haves):\n",
    "    \"\"\"\n",
    "    This function performs the various clean up tasks for the data\n",
    "    and returns the cleaned dataframe.\n",
    "    \"\"\"\n",
    "    tmp = {col:col.strip().lower() for col in list(df_part.columns)}\n",
    "    df_part = df_part.rename(columns=tmp)\n",
    "    \n",
    "    # rename using the supplied mapping\n",
    "    df_part = df_part.rename(columns=remap)\n",
    "    \n",
    "    # iterate through columns in this df partition\n",
    "    for col in df_part.columns:\n",
    "        # drop anything not in our expected list\n",
    "        if col not in must_haves:\n",
    "            df_part = df_part.drop(col, axis=1)\n",
    "            continue\n",
    "\n",
    "        # fixes datetime error found by Ty Mckercher and fixed by Paul Mahler\n",
    "        if df_part[col].dtype == 'object' and col in ['pickup_datetime', 'dropoff_datetime']:\n",
    "            df_part[col] = df_part[col].astype('datetime64[ms]')\n",
    "            continue\n",
    "\n",
    "        # if column was read as a string, recast as float\n",
    "        if df_part[col].dtype == 'object':\n",
    "            df_part[col] = df_part[col].astype('float32')\n",
    "        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "            if 'int' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('int32')\n",
    "            if 'float' in str(df_part[col].dtype):\n",
    "                df_part[col] = df_part[col].astype('float32')\n",
    "            df_part[col] = df_part[col].fillna(-1)\n",
    "            \n",
    "    return df_part\n",
    "\n",
    "def coalesce_taxi_data(fraction, random_state):\n",
    "    \"\"\"\n",
    "    This function loads and process data to form a dataframe which will be used as input for CuML algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        fraction: Fraction of axis items to return. Cannot be used with n\n",
    "        random_state: Seed for the random number generator (if int), or None. If None, a random seed will be chosen.\n",
    "                if RandomState, seed will be extracted from current state.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    base_path = 'gcs://anaconda-public-data/nyc-taxi/csv'\n",
    "\n",
    "    # list of column names that need to be re-mapped\n",
    "    remap = {}\n",
    "    remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "    remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "    remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'pickup_datetime': 'datetime64[ms]',\n",
    "     'dropoff_datetime': 'datetime64[ms]',\n",
    "     'passenger_count': 'int32',\n",
    "     'trip_distance': 'float32',\n",
    "     'pickup_longitude': 'float32',\n",
    "     'pickup_latitude': 'float32',\n",
    "     'rate_code': 'int32',\n",
    "     'dropoff_longitude': 'float32',\n",
    "     'dropoff_latitude': 'float32',\n",
    "     'fare_amount': 'float32'\n",
    "    }\n",
    "    \n",
    "    # apply a list of filter conditions to throw out records with missing or outlier values\n",
    "    query_frags = [\n",
    "        'fare_amount > 0 and fare_amount < 500',\n",
    "        'passenger_count > 0 and passenger_count < 6',\n",
    "        'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "        'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "        'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "        'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    valid_months_2016 = [str(x).rjust(2, '0') for x in range(1, 7)]\n",
    "    valid_files_2016 = [f'{base_path}/2016/yellow_tripdata_2016-{month}.csv' for month in valid_months_2016]\n",
    "    \n",
    "    df_2014_fractional = dask_cudf.read_csv(f'{base_path}/2014/yellow_*.csv', chunksize=25e6).sample(\n",
    "        frac=fraction, random_state=random_state)\n",
    "    df_2014_fractional = clean(df_2014_fractional, remap, must_haves)\n",
    "    \n",
    "    df_2015_fractional = dask_cudf.read_csv(f'{base_path}/2015/yellow_*.csv', chunksize=25e6).sample(\n",
    "        frac=fraction, random_state=random_state)\n",
    "    df_2015_fractional = clean(df_2015_fractional, remap, must_haves)\n",
    "    \n",
    "    df_2016_fractional = dask_cudf.read_csv(valid_files_2016, chunksize=25e6).sample(\n",
    "        frac=fraction, random_state=random_state)\n",
    "    df_2016_fractional = clean(df_2016_fractional, remap, must_haves)\n",
    "    \n",
    "    df_taxi = dask.dataframe.multi.concat([df_2014_fractional, df_2015_fractional, df_2016_fractional])\n",
    "    df_taxi = df_taxi.query(' and '.join(query_frags))\n",
    "    \n",
    "    return df_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-source",
   "metadata": {},
   "source": [
    "# ETL Exploration CSV vs Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "remap = {}\n",
    "remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "#create a list of columns & dtypes the df must have\n",
    "must_haves = {\n",
    " 'pickup_datetime': 'datetime64[ms]',\n",
    " 'dropoff_datetime': 'datetime64[ms]',\n",
    " 'passenger_count': 'int32',\n",
    " 'trip_distance': 'float32',\n",
    " 'pickup_longitude': 'float32',\n",
    " 'pickup_latitude': 'float32',\n",
    " 'rate_code': 'int32',\n",
    " 'dropoff_longitude': 'float32',\n",
    " 'dropoff_latitude': 'float32',\n",
    " 'fare_amount': 'float32'\n",
    "}\n",
    "\n",
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "query_frags = [\n",
    "    'fare_amount > 0 and fare_amount < 500',\n",
    "    'passenger_count > 0 and passenger_count < 6',\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "]\n",
    "\n",
    "workers = client.has_what().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-destiny",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "base_path = 'gcs://anaconda-public-data/nyc-taxi/csv'\n",
    "\n",
    "with SimpleTimer() as timer_csv:\n",
    "    # Load data into dask dataframe\n",
    "    df_csv_2014 = dask_cudf.read_csv(f'{base_path}/2014/yellow_*.csv', chunksize=25e6)\n",
    "    df_csv_2014 = clean(df_csv_2014, remap, must_haves)\n",
    "    df_csv_2014 = df_csv_2014.query(' and '.join(query_frags))\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        df_csv_2014 = client.persist(collections=df_csv_2014)\n",
    "        \n",
    "    wait(df_csv_2014)\n",
    "\n",
    "print(df_csv_2014.columns)\n",
    "rows_csv = df_csv_2014.iloc[:,0].shape[0].compute()\n",
    "print(f\"CSV load took {timer_csv.elapsed/1e9} sec. For {rows_csv} rows of data => {rows_csv/(timer_csv.elapsed/1e9)} rows/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(df_csv_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with SimpleTimer() as timer_parquet:\n",
    "    df_parquet = dask_cudf.read_parquet(f'gs://anaconda-public-data/nyc-taxi/nyc.parquet', chunksize=25e6)\n",
    "    df_parquet = clean(df_parquet, remap, must_haves)\n",
    "    df_parquet = df_parquet.query(' and '.join(query_frags))\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        df_parquet = client.persist(collections=df_parquet)\n",
    "    \n",
    "    wait(df_parquet)\n",
    "\n",
    "print(df_parquet.columns)\n",
    "rows_parquet = df_parquet.iloc[:,0].shape[0].compute()\n",
    "print(f\"Parquet load took {timer_parquet.elapsed/1e9} sec. For {rows_parquet} rows of data => {rows_parquet/(timer_parquet.elapsed/1e9)} rows/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cancel(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = (rows_parquet/(timer_parquet.elapsed/1e9))/(rows_csv/(timer_csv.elapsed/1e9))\n",
    "print(speedup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-norman",
   "metadata": {},
   "source": [
    "## Performance Validation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_elapsed_timings_to_df(df, timings, record_template, type, columns, write_to=None):\n",
    "    records = [dict(record_template, **{\"sample_index\": i,\n",
    "                                        \"elapsed\": elapsed,\n",
    "                                        \"type\": type})\n",
    "                  for i, elapsed in enumerate(timings)]\n",
    "\n",
    "    df = df.append(other=records, ignore_index=True)\n",
    "    \n",
    "    if (write_to):\n",
    "        df.to_csv(write_to, columns=columns) \n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def collect_load_time_samples(load_func, count, return_final_sample=True, verbose=False):\n",
    "    timings = []\n",
    "    for m in tqdm(range(count)):\n",
    "        with SimpleTimer() as timer:\n",
    "            df, X, y = load_func()\n",
    "        timings.append(timer.elapsed)\n",
    "    \n",
    "    if (return_final_sample):\n",
    "        return df, X, y, timings\n",
    "    \n",
    "    return None, None, None, timings\n",
    "\n",
    "\n",
    "def collect_func_time_samples(func, count, verbose=False):\n",
    "    timings = []\n",
    "    for k in tqdm(range(count)):\n",
    "        with SimpleTimer() as timer:\n",
    "            func()\n",
    "        timings.append(timer.elapsed)\n",
    "        \n",
    "    return timings\n",
    "\n",
    "\n",
    "def sweep_fit_func(model, func_id, require_compute, X, y, xy_fit, count):\n",
    "    _fit_func_attr = getattr(model, func_id)\n",
    "    if (require_compute):\n",
    "        if (xy_fit):\n",
    "            fit_func = partial(lambda X, y: _fit_func_attr(X, y).compute(), X, y)\n",
    "        else:\n",
    "            fit_func = partial(lambda X: _fit_func_attr(X).compute(), X)\n",
    "    else:\n",
    "        if (xy_fit):\n",
    "            fit_func = partial(_fit_func_attr, X, y)\n",
    "        else:\n",
    "            fit_func = partial(_fit_func_attr, X)                \n",
    "\n",
    "    return collect_func_time_samples(func=fit_func, count=count)\n",
    "\n",
    "\n",
    "def sweep_predict_func(model, func_id, require_compute, X, count):\n",
    "    _predict_func_attr = getattr(model, func_id)\n",
    "    predict_func = partial(lambda X: _predict_func_attr(X).compute(), X)\n",
    "    \n",
    "    return collect_func_time_samples(func=predict_func, count=count)\n",
    "    \n",
    "\n",
    "def performance_sweep(client, model, data_loader, hardware_type, worker_counts=[1], samples=1, load_samples=1, max_data_frac=1.0,\n",
    "                    predict_frac=0.05, scaling_type='weak', xy_fit=True, fit_requires_compute=False, update_workers_in_kwargs=True,\n",
    "                    response_dtype=np.float32, out_path='./perf_sweep.csv', append_to_existing=False, model_name=None,\n",
    "                    fit_func_id=\"fit\", predict_func_id=\"predict\", scaling_denom=None, model_args={}, model_kwargs={}):\n",
    "    \"\"\"\n",
    "    Primary performance sweep entrypoint.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    client: DASK client associated with the cluster we're interesting in collecting performance data for.\n",
    "    \n",
    "    model: Model object on which to gather performance data. This will be created and destroyed,\n",
    "        once for each element of 'worker_counts'\n",
    "    \n",
    "    data_loader: arbitrary data loading function that will be called to load the appropriate testing data.\n",
    "        Function that is responsible for loading and returning the data to be used for a given performance run. Function\n",
    "        signature must accept (client, fraction, and random_state). Client should be used to distribute data, and loaders\n",
    "        should utilize fraction and random_state with dask's dataframe.sample method to allow for control of how much data\n",
    "        is loaded.\n",
    "        \n",
    "        When called, its return value should be of the form: df, X, y, where df is the full dask_cudf dataframe, X is a\n",
    "        dask_cudf dataframe which contains all explanatory variables that will be passed to the 'fit' function, and y is a\n",
    "        dask_cudf series or dataframe that contains response variables which should be passed to fit/predict as fit(X, y)\n",
    "    \n",
    "    hardware_type: indicates the core hardware the current sweep is running on. ex. 'T4', 'V100', 'A100'\n",
    "    \n",
    "    worker_counts: List indicating the number of workers that should be swept. Ex [1, 2, 4]\n",
    "        worker counts must fit within the cluster associated with 'client', if the current DASK worker count is different\n",
    "        from what is requested on a given sweep, attempt to automatically scale the worker count. NOTE: this does not \n",
    "        mean we will scale the available cluster nodes, just the number of deployed worker pods.\n",
    "    \n",
    "    samples: number of fit/predict samples to record per worker count\n",
    "    \n",
    "    load_samples: number of times to sample data loads. This effectively times how long 'data_loader' runs.\n",
    "    \n",
    "    max_data_frac: maximum fraction of data to return.\n",
    "        Strong scaling: each run will utilize max_data_frac data.\n",
    "        Weak scaling: each run will utilize (current worker count) / (max worker count) * max_data_frac data.\n",
    "        \n",
    "    predict_frac: fraction of training data used to test inference\n",
    "    \n",
    "    scaling_type: values can be 'weak' or 'strong' indicating the type of scaling sweep to perform.\n",
    "    \n",
    "    xy_fit: indicates whether or not the model's 'fit' function is of the form (X, y), when xy_fit is False, we assume that\n",
    "        fit is of the form (X), as is the case with various unsupervised methods ex. KNN.\n",
    "    \n",
    "    fit_requires_compute: False generally, set this to True if the model's 'fit' function requires a corresponding '.compute()'\n",
    "        call to execute the required work.\n",
    "    \n",
    "    update_workers_in_kwargs: Some algorithms accept a 'workers' list, much like DASK, and will require their kwargs to have\n",
    "        workers populated. Setting this flag handles this automatically.\n",
    "        \n",
    "    response_dtype: defaults to np.float32, some algorithms require another dtype, such as int32\n",
    "    \n",
    "    out_path: path where performance data csv should be saved\n",
    "    \n",
    "    append_to_existing: When true, append results to an existing csv, otherwise overwrite.\n",
    "    \n",
    "    model_name: Override what we output as the model name\n",
    "    \n",
    "    fit_func_id: Defaults to 'fit', only set this if the model has a non-standard naming.\n",
    "    \n",
    "    predict_func_id: Defaults to 'predict', only set this if the model has a on-standard predict naming.\n",
    "    \n",
    "    scaling_denom: (weak scaling) defaults to max(workers) if unset. Specifies the maximum worker count that weak scaling\n",
    "        should scale against. For example, when using 1 worker in a weak scaling sweep, the worker will attempt to\n",
    "        process a fraction of the total data equal to 1/scaling_denom\n",
    "    \n",
    "    model_args: args that will be passed to the model's constructor\n",
    "    \n",
    "    model_kwargs: keyword args that will be passed to the model's constructor\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cols = ['n_workers', 'sample_index', 'elapsed', 'type', 'algorithm', 'scaling_type', 'data_fraction', 'hardware']\n",
    "    perf_df = cudf.DataFrame(columns=cols)\n",
    "    if (append_to_existing):\n",
    "        try:\n",
    "            perf_df = cudf.read_csv(out_path)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    model_name = model_name if model_name else str(model)\n",
    "    scaling_denom = scaling_denom if (scaling_denom is not None) else max(worker_counts)\n",
    "    max_data_frac = min(1.0, max_data_frac)\n",
    "\n",
    "    start_msg = f\"Starting {scaling_type}-scaling performance sweep for:\\n\"\n",
    "    start_msg += f\" model      : {model_name}\\n\"\n",
    "    start_msg += f\" data loader: {data_loader}.\\n\"\n",
    "    start_msg += f\"Configuration\\n\"\n",
    "    start_msg += \"==========================\\n\"\n",
    "    start_msg += f\"{'Worker counts':<25} : {worker_counts}\\n\"\n",
    "    start_msg += f\"{'Fit/Predict samples':<25} : {samples}\\n\"\n",
    "    start_msg += f\"{'Data load samples':<25} : {load_samples}\\n\"\n",
    "    start_msg += f\"- {'Max data fraction':<23} : {max_data_frac}\\n\"\n",
    "    start_msg += f\"{'Model fit':<25} : {'X ~ y' if xy_fit else 'X'}\\n\"\n",
    "    start_msg += f\"- {'Response DType':<23} : {response_dtype}\\n\"\n",
    "    start_msg += f\"{'Writing results to':<25} : {out_path}\\n\"\n",
    "    start_msg += f\"- {'Method':<23} : {'overwrite' if not append_to_existing else 'append'}\\n\"\n",
    "    print(start_msg, flush=True)\n",
    "    \n",
    "    for n in worker_counts:        \n",
    "        fraction = (n / scaling_denom) * max_data_frac if scaling_type == 'weak' else max_data_frac\n",
    "        record_template = {\"n_workers\": n, \"type\": \"predict\", \"algorithm\": model_name,\n",
    "               \"scaling_type\": scaling_type, \"data_fraction\": fraction, \"hardware\": hardware_type}\n",
    "        scale_workers(client, n)\n",
    "\n",
    "        print(f\"Sampling <{load_samples}> load times with {n} workers.\", flush=True)\n",
    "        \n",
    "        load_func = partial(data_loader, client=client, response_dtype=response_dtype, fraction=fraction, random_state=0)\n",
    "        df, X, y, load_timings = collect_load_time_samples(load_func=load_func, count=load_samples)\n",
    "        \n",
    "        perf_df = record_elapsed_timings_to_df(df=perf_df, timings=load_timings, type='load',\n",
    "                                                    record_template=record_template, columns=cols, write_to=out_path)\n",
    "\n",
    "        print(f\"Finished loading <{load_samples}>, samples, to <{n}> workers with a mean time of {np.mean(load_timings)/1e9:0.4f} sec.\", flush=True)\n",
    "        print(f\"Sweeping {model_name} '{fit_func_id}' with <{n}> workers. Sampling <{samples}> times.\", flush=True)\n",
    "\n",
    "        if (update_workers_in_kwargs and 'workers' in model_kwargs):\n",
    "            model_kwargs['workers'] = workers = client.has_what().keys()\n",
    "    \n",
    "        m = model(*model_args, **model_kwargs)\n",
    "        if (fit_func_id):\n",
    "            fit_timings = sweep_fit_func(model=m, func_id=fit_func_id,\n",
    "                                             require_compute=fit_requires_compute,\n",
    "                                             X=X, y=y, xy_fit=xy_fit, count=samples)\n",
    "\n",
    "            perf_df = record_elapsed_timings_to_df(df=perf_df, timings=fit_timings, type='fit',\n",
    "                                                        record_template=record_template, columns=cols, write_to=out_path)\n",
    "\n",
    "            print(f\"Finished gathering <{samples}>, 'fit' samples using <{n}> workers, with a mean time of {np.mean(fit_timings)/1e9:0.4f} sec.\",\n",
    "                  flush=True)\n",
    "        else:\n",
    "            print(f\"Skipping fit sweep, fit_func_id is None\")\n",
    "\n",
    "        if (predict_func_id):\n",
    "            print(f\"Sweeping {model_name} '{predict_func_id}' with <{n}> workers. Sampling <{samples}> times.\", flush=True)\n",
    "            predict_timings = sweep_predict_func(model=m, func_id=predict_func_id,\n",
    "                                                     require_compute=True, X=X, count=samples)\n",
    "\n",
    "            perf_df = record_elapsed_timings_to_df(df=perf_df, timings=predict_timings, type='predict',\n",
    "                                                        record_template=record_template, columns=cols, write_to=out_path)\n",
    "            \n",
    "            print(f\"Finished gathering <{samples}>, 'predict' samples using <{n}> workers, with a mean time of {np.mean(predict_timings)/1e9:0.4f} sec.\",\n",
    "                  flush=True)\n",
    "        else:\n",
    "            print(f\"Skipping inference sweep. predict_func_id is None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-crest",
   "metadata": {},
   "source": [
    "### Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ci(df, fields, groupby):\n",
    "    gbdf = df[fields].groupby(groupby).agg(['mean', 'std', 'count'])   \n",
    "    \n",
    "    ci = (1.96 + gbdf['elapsed']['std'] / np.sqrt(gbdf['elapsed']['count']))\n",
    "    \n",
    "    ci_df = ci.reset_index()\n",
    "    ci_df['ci.low'] = gbdf['elapsed'].reset_index()['mean'] - ci_df[0]\n",
    "    ci_df['ci.high'] = gbdf['elapsed'].reset_index()['mean'] + ci_df[0]\n",
    "    \n",
    "    return ci_df\n",
    "\n",
    "def visualize_csv_data(csv_path):\n",
    "    df = cudf.read_csv(csv_path)\n",
    "    \n",
    "    fields = ['elapsed', 'elapsed_sec', 'type', 'n_workers', 'hardware', 'scaling_type']\n",
    "    groupby = ['n_workers', 'type', 'hardware', 'scaling_type']\n",
    "    df['elapsed_sec'] = df['elapsed']/1e9\n",
    "\n",
    "    ci_df = simple_ci(df, fields, groupby=groupby)\n",
    "\n",
    "    # Rescale to seconds\n",
    "    ci_df[['ci.low', 'ci.high']] = ci_df[['ci.low', 'ci.high']]/1e9\n",
    "\n",
    "    # Print confidence intervals\n",
    "    print(ci_df[['hardware', 'n_workers', 'type', 'ci.low', 'ci.high']][ci_df['type'] != 'load'])\n",
    "\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    sns.set(rc={'figure.figsize':(20, 10)}, font_scale=2)\n",
    "\n",
    "    # Boxplots for elapsed time at each worker count.\n",
    "    plot_df = df[fields][df[fields].type != 'load'].to_pandas()\n",
    "    ax = sns.catplot(data=plot_df, x=\"n_workers\", y=\"elapsed_sec\",\n",
    "                     col=\"type\", row=\"scaling_type\", hue=\"hardware\", kind=\"box\",\n",
    "                     height=8, order=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-agreement",
   "metadata": {},
   "source": [
    "### Taxi Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxi_csv_data_loader(client, response_dtype=np.float32, fraction=1.0, random_state=0):\n",
    "    \"\"\"\n",
    "    A CSV data_loader. Reads CSV files, clean and process to return a dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    client: DASK client associated with the cluster we're interesting in collecting performance data for.\n",
    "    fraction: Fraction of axis items to return. Cannot be used with n\n",
    "    random_state: Seed for the random number generator (if int), or None. If None, a random seed will be chosen.\n",
    "                if RandomState, seed will be extracted from current state.\n",
    "\n",
    "    \"\"\"\n",
    "    response_id = 'fare_amount'\n",
    "    workers = client.has_what().keys()\n",
    "    km_fields = ['passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                 'dropoff_longitude', 'dropoff_latitude', 'fare_amount']\n",
    "    \n",
    "    taxi_df = coalesce_taxi_data(fraction=fraction, random_state=random_state)\n",
    "    \n",
    "    taxi_df = taxi_df[km_fields]\n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        taxi_df = client.persist(collections=taxi_df)\n",
    "    \n",
    "    X = taxi_df[taxi_df.columns.difference([response_id])].astype(np.float32)\n",
    "    y = taxi_df[response_id].astype(response_dtype)\n",
    "    \n",
    "    wait(taxi_df)\n",
    "    \n",
    "    return taxi_df, X, y\n",
    "\n",
    "def taxi_parquet_data_loader(client, response_dtype=np.float32, fraction=1.0, random_state=0):\n",
    "    \"\"\"\n",
    "    A Parquet data_loader. Reads parquet files, clean and process to return a dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    client: DASK client associated with the cluster we're interesting in collecting performance data for.\n",
    "    fraction: Fraction of axis items to return. Cannot be used with n\n",
    "    random_state: Seed for the random number generator (if int), or None. If None, a random seed will be chosen.\n",
    "                if RandomState, seed will be extracted from current state.\n",
    "\n",
    "    \"\"\"\n",
    "    # list of column names that need to be re-mapped\n",
    "    remap = {}\n",
    "    remap['tpep_pickup_datetime'] = 'pickup_datetime'\n",
    "    remap['tpep_dropoff_datetime'] = 'dropoff_datetime'\n",
    "    remap['ratecodeid'] = 'rate_code'\n",
    "\n",
    "    #create a list of columns & dtypes the df must have\n",
    "    must_haves = {\n",
    "     'pickup_datetime': 'datetime64[ms]',\n",
    "     'dropoff_datetime': 'datetime64[ms]',\n",
    "     'passenger_count': 'int32',\n",
    "     'trip_distance': 'float32',\n",
    "     'pickup_longitude': 'float32',\n",
    "     'pickup_latitude': 'float32',\n",
    "     'rate_code': 'int32',\n",
    "     'dropoff_longitude': 'float32',\n",
    "     'dropoff_latitude': 'float32',\n",
    "     'fare_amount': 'float32'\n",
    "    }\n",
    "\n",
    "    # apply a list of filter conditions to throw out records with missing or outlier values\n",
    "    query_frags = [\n",
    "        'fare_amount > 0 and fare_amount < 500',\n",
    "        'passenger_count > 0 and passenger_count < 6',\n",
    "        'pickup_longitude > -75 and pickup_longitude < -73',\n",
    "        'dropoff_longitude > -75 and dropoff_longitude < -73',\n",
    "        'pickup_latitude > 40 and pickup_latitude < 42',\n",
    "        'dropoff_latitude > 40 and dropoff_latitude < 42'\n",
    "    ]\n",
    "\n",
    "    workers = client.has_what().keys()\n",
    "    taxi_parquet_path = \"gs://anaconda-public-data/nyc-taxi/nyc.parquet\"\n",
    "    response_id = 'fare_amount'\n",
    "    fields = ['passenger_count', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'rate_code',\n",
    "                 'dropoff_longitude', 'dropoff_latitude', 'fare_amount']\n",
    "    \n",
    "    taxi_df = dask_cudf.read_parquet(taxi_parquet_path, npartitions=len(workers))\n",
    "    taxi_df = clean(taxi_df, remap, must_haves)\n",
    "    taxi_df = taxi_df.query(' and '.join(query_frags))\n",
    "    taxi_df = taxi_df[fields]\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        taxi_df = client.persist(collections=taxi_df)\n",
    "    \n",
    "    wait(taxi_df)\n",
    "\n",
    "    X = taxi_df[taxi_df.columns.difference([response_id])].astype(np.float32)\n",
    "    y = taxi_df[response_id].astype(response_dtype)\n",
    "        \n",
    "    return taxi_df, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-tampa",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a weighted vote of their predictions. There are three subsets of ensemble learning methods. \n",
    "\n",
    "1. __BAGGing__, or __B__ootstrap __AGG__regating\n",
    "2. __Boosting__\n",
    "3. __Stacking__\n",
    "\n",
    "If you like to get more details about it, please refer to [Simple Guide to Ensemble Methods](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2)\n",
    "\n",
    "\n",
    "### Random Forest Regressor\n",
    "\n",
    "The Decision Tree algorithm has a major disadvantage in that it causes over-fitting. To address these weaknesses, we turn to Random Forest. Random forest is a Supervised Learning algorithm which uses ensemble learning method for classification and regression. It is very fast and robust than other models. For anyone interested, we have added original papers and couple of blogs to learn more about Random Forest Regressor.\n",
    "\n",
    "We are going to use [RandomForestRegressor API](https://docs.rapids.ai/api/cuml/stable/api.html#random-forest) from CuML library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-entertainment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cuml.dask.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_kwargs = {\n",
    "    \"workers\": client.has_what().keys(),\n",
    "    \"n_estimators\": 10,\n",
    "    \"max_depth\": 12\n",
    "}\n",
    "rf_csv_path = f\"./{out_prefix}_random_forest_regression.csv\"\n",
    "\n",
    "performance_sweep(client=client, model=RandomForestRegressor,\n",
    "                **sweep_kwargs,\n",
    "                out_path=rf_csv_path,\n",
    "                response_dtype=np.int32,\n",
    "                model_kwargs=rf_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_csv_path = f\"./{out_prefix}_random_forest_regression.csv\"\n",
    "visualize_csv_data(rf_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-eugene",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost falls under the category of Boosting techniques in Ensemble Learning. The algorithm was developed to efficiently reduce computing time and allocate an optimal usage of memory resources. Important features of implementation include handling of missing values (Sparse Aware), Block Structure to support parallelization in tree construction and the ability to fit and boost on new data added to a trained model. ([reference](https://www.kdnuggets.com/2017/10/xgboost-top-machine-learning-method-kaggle-explained.html)) \n",
    "\n",
    "Here is the original paper,\n",
    "[XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xg_args = [client]\n",
    "xg_kwargs = {\n",
    "    'params': {\n",
    "        'tree_method': 'gpu_hist',\n",
    "    },\n",
    "    'num_boost_round': 100\n",
    "}\n",
    "\n",
    "xgb_csv_path = f'./{out_prefix}_xgb.csv'\n",
    "\n",
    "class XGBProxy():\n",
    "    \"\"\"\n",
    "    Create a simple API wrapper around XGBoost so that it supports the fit/predict workflow.\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    data_loader: data loader object intended to be used by the performance sweep.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_loader):\n",
    "        self.args = []\n",
    "        self.kwargs = {}\n",
    "        self.data_loader = data_loader\n",
    "        self.trained_model = None\n",
    "        \n",
    "    def loader(self, client, response_dtype, fraction, random_state):\n",
    "        \"\"\"\n",
    "        Wrap the data loader method so that it creates a DMatrix from the returned data.\n",
    "        \"\"\"\n",
    "        df, X, y = self.data_loader(client, response_dtype, fraction, random_state)\n",
    "        dmatrix = xgb.dask.DaskDMatrix(client, X, y)\n",
    "        \n",
    "        return dmatrix, dmatrix, dmatrix\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Acts as a pseudo init function which initializes our model args.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Wrap dask.train, and store the model on our proxy object.\n",
    "        \"\"\"\n",
    "        if (self.trained_model):\n",
    "            del self.trained_model\n",
    "            \n",
    "        self.trained_model = xgb.dask.train(*self.args,\n",
    "                              dtrain=X,\n",
    "                              evals=[(X, 'train')],\n",
    "                              **self.kwargs)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        assert(self.trained_model)\n",
    "        \n",
    "        return xgb.dask.predict(*self.args, self.trained_model, X)\n",
    "    \n",
    "\n",
    "xgb_proxy = XGBProxy(data_loader)\n",
    "performance_sweep(client=client, model=xgb_proxy, data_loader=xgb_proxy.loader, hardware_type=hardware_type,\n",
    "                worker_counts=worker_counts, \n",
    "                samples=samples,\n",
    "                load_samples=load_samples,\n",
    "                max_data_frac=max_data_frac, \n",
    "                scaling_type=scale_type,\n",
    "                out_path=xgb_csv_path,\n",
    "                append_to_existing=append_to_existing,\n",
    "                update_workers_in_kwargs=False,\n",
    "                xy_fit=False,\n",
    "                scaling_denom = scaling_denom,\n",
    "                model_args=xg_args,\n",
    "                model_kwargs=xg_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-patch",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "\n",
    "##### Dask\n",
    "\n",
    "##### XGBoost\n",
    "* [Ensemble Learning to Improve Machine Learning Results](https://blog.statsbot.co/ensemble-learning-d1dcd548e936)\n",
    "* [Complete Guide to Parameter Tuning in XGBoost with codes in Python](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
    "* [Understanding XGBoost Algorithm In Detail](https://analyticsindiamag.com/xgboost-internal-working-to-make-decision-trees-and-deduce-predictions/)\n",
    "\n",
    "##### Random Forest Regressor\n",
    "* [Random Forest](https://williamkoehrsen.medium.com/random-forest-simple-explanation-377895a60d2d) \\\n",
    "* [Random Forest Regression](https://towardsdatascience.com/machine-learning-basics-random-forest-regression-be3e1e3bb91a)\n",
    "* [Classification and Regression by randomForest](https://www.researchgate.net/profile/Andy-Liaw/publication/228451484_Classification_and_Regression_by_RandomForest/links/53fb24cc0cf20a45497047ab/Classification-and-Regression-by-RandomForest.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-scope",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
