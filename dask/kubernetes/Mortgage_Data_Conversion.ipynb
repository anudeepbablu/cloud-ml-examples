{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-salmon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import certifi\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy as cp\n",
    "import gcsfs\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import uuid\n",
    "import yaml\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from math import cos, sin, asin, sqrt, pi\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "import dask_cudf\n",
    "\n",
    "from dask_kubernetes import KubeCluster, make_pod_from_dict\n",
    "from dask.distributed import Client, WorkerPlugin, wait, progress, get_worker\n",
    "\n",
    "from cuml import ForestInference\n",
    "\n",
    "\n",
    "class SimpleTimer:\n",
    "    def __init__(self):\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.elapsed = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter_ns()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time.perf_counter_ns()\n",
    "        self.elapsed = self.end - self.start\n",
    "\n",
    "        \n",
    "def create_pod_from_yaml(yaml_file):\n",
    "    with open(yaml_file, 'r') as reader:\n",
    "        d = yaml.safe_load(reader)\n",
    "        d = dask.config.expand_environment_variables(d)\n",
    "    return make_pod_from_dict(d)\n",
    "\n",
    "\n",
    "def build_worker_and_scheduler_pods(sched_spec, worker_spec):\n",
    "    assert os.path.isfile(sched_spec)\n",
    "    assert os.path.isfile(worker_spec)\n",
    "\n",
    "    sched_pod = create_pod_from_yaml(sched_spec)\n",
    "    worker_pod = create_pod_from_yaml(worker_spec)\n",
    "\n",
    "    return sched_pod, worker_pod\n",
    "\n",
    "\n",
    "dask.config.set({\"logging.kubernetes\": \"info\",\n",
    "                 \"logging.distributed\": \"info\",\n",
    "                 \"kubernetes.scheduler-service-type\": \"LoadBalancer\",\n",
    "                 \"kubernetes.idle-timeout\": None,\n",
    "                 \"kubernetes.scheduler-service-wait-timeout\": 3600,\n",
    "                 \"kubernetes.deploy-mode\": \"remote\",\n",
    "                 \"kubernetes.logging\": \"info\",\n",
    "                 \"distributed.logging\": \"info\",\n",
    "                 \"distributed.scheduler.idle-timeout\": None,\n",
    "                 \"distributed.scheduler.locks.lease-timeout\": None,\n",
    "                 \"distributed.comm.timeouts.connect\": 3600,\n",
    "                 \"distributed.comm.tls.ca-file\": certifi.where()})\n",
    "\n",
    "YOUR_GCP_PROJECT ='<project_id>'\n",
    "YOUR_GCP_TOKEN = '/etc/secrets/keyfile.json'\n",
    "YOUR_MORTGAGE_DATA_PATH = 'gs://<path_to_file>'\n",
    "YOUR_STORAGE_OPTS = {\n",
    "    'token': YOUR_GCP_TOKEN # This will be the service account keyfile\n",
    "}\n",
    "\n",
    "# This should be the path where you have uploaded the full mortgage CSV data.\n",
    "# - names.csv, perf, acq\n",
    "YOUR_MORTGAGE_DATA_DIR = \"gs://<mortgage_folder_path>/mortgage_large_csv\"\n",
    "\n",
    "# Where you want to land the converted Parquet files\n",
    "YOUR_OUTPUT_PATH = \"gs://<mortgage_out_folder_path>\"\n",
    "\n",
    "sched_spec_path = \"./specs/sched-spec-custom.yaml\"\n",
    "worker_spec_path = \"./specs/worker-spec-custom.yaml\"\n",
    "\n",
    "sched_pod, worker_pod = build_worker_and_scheduler_pods(sched_spec=sched_spec_path,\n",
    "                                                        worker_spec=worker_spec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = KubeCluster(pod_template=worker_pod,\n",
    "                      scheduler_pod_template=sched_pod)\n",
    "\n",
    "client = Client(cluster)\n",
    "scheduler_address = cluster.scheduler_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(client, n_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-sudan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_names_csv(col_names_path, storage_opts):\n",
    "    \"\"\" \n",
    "    Loads names used for renaming the banks\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\"seller_name\", \"new\"]\n",
    "\n",
    "    dtypes = OrderedDict([\n",
    "        (\"seller_name\", \"str\"),\n",
    "        (\"new\", \"str\")\n",
    "    ])\n",
    "\n",
    "    df_names = dask_cudf.read_csv(\n",
    "        col_names_path,\n",
    "        names=cols,\n",
    "        delimiter=\"|\",\n",
    "        dtype=list(dtypes.values()),\n",
    "        skiprows=1,\n",
    "        storage_options=storage_opts\n",
    "    )\n",
    "        \n",
    "    return df_names\n",
    "\n",
    "def load_acquisition_csv(fpath, storage_opts):\n",
    "    \"\"\" \n",
    "    Loads acquisition data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        \"loan_id\",\n",
    "        \"orig_channel\",\n",
    "        \"seller_name\",\n",
    "        \"orig_interest_rate\",\n",
    "        \"orig_upb\",\n",
    "        \"orig_loan_term\",\n",
    "        \"orig_date\",\n",
    "        \"first_pay_date\",\n",
    "        \"orig_ltv\",\n",
    "        \"orig_cltv\",\n",
    "        \"num_borrowers\",\n",
    "        \"dti\",\n",
    "        \"borrower_credit_score\",\n",
    "        \"first_home_buyer\",\n",
    "        \"loan_purpose\",\n",
    "        \"property_type\",\n",
    "        \"num_units\",\n",
    "        \"occupancy_status\",\n",
    "        \"property_state\",\n",
    "        \"zip\",\n",
    "        \"mortgage_insurance_percent\",\n",
    "        \"product_type\",\n",
    "        \"coborrow_credit_score\",\n",
    "        \"mortgage_insurance_type\",\n",
    "        \"relocation_mortgage_indicator\",\n",
    "    ]\n",
    "\n",
    "    dtypes = OrderedDict(\n",
    "        [\n",
    "            (\"loan_id\", \"int64\"),\n",
    "            (\"orig_channel\", \"str\"),\n",
    "            (\"seller_name\", \"str\"),\n",
    "            (\"orig_interest_rate\", \"float64\"),\n",
    "            (\"orig_upb\", \"int64\"),\n",
    "            (\"orig_loan_term\", \"int64\"),\n",
    "            (\"orig_date\", \"date\"),\n",
    "            (\"first_pay_date\", \"date\"),\n",
    "            (\"orig_ltv\", \"float64\"),\n",
    "            (\"orig_cltv\", \"float64\"),\n",
    "            (\"num_borrowers\", \"float64\"),\n",
    "            (\"dti\", \"float64\"),\n",
    "            (\"borrower_credit_score\", \"float64\"),\n",
    "            (\"first_home_buyer\", \"str\"),\n",
    "            (\"loan_purpose\", \"str\"),\n",
    "            (\"property_type\", \"str\"),\n",
    "            (\"num_units\", \"int64\"),\n",
    "            (\"occupancy_status\", \"str\"),\n",
    "            (\"property_state\", \"str\"),\n",
    "            (\"zip\", \"int64\"),\n",
    "            (\"mortgage_insurance_percent\", \"float64\"),\n",
    "            (\"product_type\", \"str\"),\n",
    "            (\"coborrow_credit_score\", \"float64\"),\n",
    "            (\"mortgage_insurance_type\", \"float64\"),\n",
    "            (\"relocation_mortgage_indicator\", \"str\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(fpath)\n",
    "    df_acq = dask_cudf.read_csv(\n",
    "        fpath,\n",
    "        names=cols,\n",
    "        delimiter=\"|\",\n",
    "        dtype=list(dtypes.values()),\n",
    "        skiprows=1,\n",
    "        storage_options=storage_opts\n",
    "    )\n",
    "    \n",
    "    return df_acq\n",
    "\n",
    "def load_performance_csv(fpath, storage_opts):\n",
    "    \"\"\" \n",
    "    Loads performance data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GPU DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        \"loan_id\",\n",
    "        \"monthly_reporting_period\",\n",
    "        \"servicer\",\n",
    "        \"interest_rate\",\n",
    "        \"current_actual_upb\",\n",
    "        \"loan_age\",\n",
    "        \"remaining_months_to_legal_maturity\",\n",
    "        \"adj_remaining_months_to_maturity\",\n",
    "        \"maturity_date\",\n",
    "        \"msa\",\n",
    "        \"current_loan_delinquency_status\",\n",
    "        \"mod_flag\",\n",
    "        \"zero_balance_code\",\n",
    "        \"zero_balance_effective_date\",\n",
    "        \"last_paid_installment_date\",\n",
    "        \"foreclosed_after\",\n",
    "        \"disposition_date\",\n",
    "        \"foreclosure_costs\",\n",
    "        \"prop_preservation_and_repair_costs\",\n",
    "        \"asset_recovery_costs\",\n",
    "        \"misc_holding_expenses\",\n",
    "        \"holding_taxes\",\n",
    "        \"net_sale_proceeds\",\n",
    "        \"credit_enhancement_proceeds\",\n",
    "        \"repurchase_make_whole_proceeds\",\n",
    "        \"other_foreclosure_proceeds\",\n",
    "        \"non_interest_bearing_upb\",\n",
    "        \"principal_forgiveness_upb\",\n",
    "        \"repurchase_make_whole_proceeds_flag\",\n",
    "        \"foreclosure_principal_write_off_amount\",\n",
    "        \"servicing_activity_indicator\",\n",
    "    ]\n",
    "\n",
    "    dtypes = OrderedDict(\n",
    "        [\n",
    "            (\"loan_id\", \"int64\"),\n",
    "            (\"monthly_reporting_period\", \"date\"),\n",
    "            (\"servicer\", \"str\"),\n",
    "            (\"interest_rate\", \"float64\"),\n",
    "            (\"current_actual_upb\", \"float64\"),\n",
    "            (\"loan_age\", \"float64\"),\n",
    "            (\"remaining_months_to_legal_maturity\", \"float64\"),\n",
    "            (\"adj_remaining_months_to_maturity\", \"float64\"),\n",
    "            (\"maturity_date\", \"date\"),\n",
    "            (\"msa\", \"float64\"),\n",
    "            (\"current_loan_delinquency_status\", \"int32\"),\n",
    "            (\"mod_flag\", \"str\"),\n",
    "            (\"zero_balance_code\", \"str\"),\n",
    "            (\"zero_balance_effective_date\", \"date\"),\n",
    "            (\"last_paid_installment_date\", \"date\"),\n",
    "            (\"foreclosed_after\", \"date\"),\n",
    "            (\"disposition_date\", \"date\"),\n",
    "            (\"foreclosure_costs\", \"float64\"),\n",
    "            (\"prop_preservation_and_repair_costs\", \"float64\"),\n",
    "            (\"asset_recovery_costs\", \"float64\"),\n",
    "            (\"misc_holding_expenses\", \"float64\"),\n",
    "            (\"holding_taxes\", \"float64\"),\n",
    "            (\"net_sale_proceeds\", \"float64\"),\n",
    "            (\"credit_enhancement_proceeds\", \"float64\"),\n",
    "            (\"repurchase_make_whole_proceeds\", \"float64\"),\n",
    "            (\"other_foreclosure_proceeds\", \"float64\"),\n",
    "            (\"non_interest_bearing_upb\", \"float64\"),\n",
    "            (\"principal_forgiveness_upb\", \"float64\"),\n",
    "            (\"repurchase_make_whole_proceeds_flag\", \"str\"),\n",
    "            (\"foreclosure_principal_write_off_amount\", \"float64\"),\n",
    "            (\"servicing_activity_indicator\", \"str\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(fpath)\n",
    "    df_perf = dask_cudf.read_csv(\n",
    "        fpath,\n",
    "        names=cols,\n",
    "        delimiter=\"|\",\n",
    "        dtype=list(dtypes.values()),\n",
    "        skiprows=1,\n",
    "        storage_options=storage_opts\n",
    "    )\n",
    "    \n",
    "    return df_perf\n",
    "\n",
    "def process_acq(df_names, fpath, storage_opts):\n",
    "    df_acq = load_acquisition_csv(fpath, storage_opts)\n",
    "    \n",
    "    #Normalize seller names\n",
    "    df_acq = df_acq.merge(df_names, how=\"left\", on=[\"seller_name\"])\n",
    "    df_acq[\"seller_name\"] = df_acq[\"new\"]\n",
    "    df_acq.drop(columns=\"new\")\n",
    "        \n",
    "    return df_acq\n",
    "\n",
    "\n",
    "def process_perf(df_acq, fpath, storage_opts):\n",
    "    df_perf = load_performance_csv(fpath, storage_opts)\n",
    "    \n",
    "    return df_perf\n",
    "\n",
    "def ingest_and_convert(start_year, end_year, data_dir, client, filesystem, storage_opts):\n",
    "    \"\"\"\n",
    "    Driver function for the ETL step\n",
    "    \n",
    "    Iterates through all files in `data_dir` between `start_year` \n",
    "    and `end_year` and calls the ETL function for each file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dask futures to arrow tables containing post ETL data for all processed files.\n",
    "    \"\"\"\n",
    "    print(\"Starting ETL\")\n",
    "    etl_start = time.perf_counter_ns()\n",
    "    workers = client.has_what().keys()\n",
    "    \n",
    "    path_names = f\"{data_dir}/names.csv\"\n",
    "    path_perf = f\"{data_dir}/perf\"\n",
    "    path_acq = f\"{data_dir}/acq\"\n",
    "    \n",
    "    # Pre-load name map\n",
    "    df_names = load_names_csv(col_names_path=path_names, storage_opts=storage_opts)\n",
    "    \n",
    "    perf_dfs = []\n",
    "    acq_dfs = []\n",
    "    for year in range(start_year, end_year+1):\n",
    "        for quarter in range(1, 4+1):\n",
    "            # Pre-load acquisition data for the current year/quarter\n",
    "            acq = f\"{path_acq}/Acquisition_{year}Q{quarter}.txt\"\n",
    "            acq_dfs.extend([process_acq(df_names, acq, storage_opts)])\n",
    "            \n",
    "            # Find all performance files for the current year/quarter\n",
    "            globstr = f\"{path_perf}/Performance_{year}Q{quarter}*\"\n",
    "            files = filesystem.glob(globstr)\n",
    "            \n",
    "            for file in files:\n",
    "                # Process each performance file and combine them into a single dataframe\n",
    "                perf_dfs.append(process_perf(acq_dfs[-1], f\"gs://{file}\", storage_opts))\n",
    "    \n",
    "    df_acq = dask.dataframe.multi.concat(acq_dfs)\n",
    "    df_perf = dask.dataframe.multi.concat(perf_dfs)\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        df_acq = client.persist(collections=df_acq)\n",
    "    \n",
    "    wait(df_acq)\n",
    "    df_acq.to_parquet(f\"{YOUR_OUTPUT_PATH}/mortgage_acq.parquet\")\n",
    "    del df_acq\n",
    "    \n",
    "    with dask.annotate(workers=set(workers)):\n",
    "        df_perf = client.persist(collections=df_perf)\n",
    "    \n",
    "    wait(df_perf)\n",
    "    df_perf.to_parquet(f\"{YOUR_OUTPUT_PATH}/mortgage_perf.parquet\")\n",
    "    del df_perf\n",
    "    \n",
    "    etl_elapsed_ns = time.perf_counter_ns() - etl_start\n",
    "    print(f\"Conversion took: {etl_elapsed_ns/1e9:0.2f} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-strategy",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_fs = gcsfs.core.GCSFileSystem(project=YOUR_PROJECT_ID)\n",
    "\n",
    "ingest_and_convert(start_year=2000, end_year=2016, data_dir=YOUR_MORTGAGE_DATA_DIR, client=client,\n",
    "                   filesystem=gcs_fs, storage_opts=YOUR_STORAGE_OPTS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
